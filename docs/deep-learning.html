<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Topic 6 Deep Learning | A Tour of Machine Learning Tools</title>
  <meta name="description" content="A tour of selected machine learning tools that are used in plant pathogenomics research" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Topic 6 Deep Learning | A Tour of Machine Learning Tools" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A tour of selected machine learning tools that are used in plant pathogenomics research" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Topic 6 Deep Learning | A Tour of Machine Learning Tools" />
  
  <meta name="twitter:description" content="A tour of selected machine learning tools that are used in plant pathogenomics research" />
  

<meta name="author" content="Dan MacLean" />


<meta name="date" content="2021-04-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="supervised-learning.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intro to ML</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Setting up</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a><ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#knowledge-prerequisites"><i class="fa fa-check"></i><b>1.1.1</b> Knowledge prerequisites</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#software-prerequisites"><i class="fa fa-check"></i><b>1.1.2</b> Software prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#installing-r"><i class="fa fa-check"></i><b>1.2</b> Installing R</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#installing-rstudio"><i class="fa fa-check"></i><b>1.3</b> Installing RStudio</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#installing-r-packages-in-rstudio"><i class="fa fa-check"></i><b>1.4</b> Installing R packages in RStudio</a><ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#standard-packages"><i class="fa fa-check"></i><b>1.4.1</b> Standard packages</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="motivation.html"><a href="motivation.html"><i class="fa fa-check"></i><b>2</b> Motivation</a><ul>
<li class="chapter" data-level="2.1" data-path="motivation.html"><a href="motivation.html#open-the-pod-bay-doors-please-hal."><i class="fa fa-check"></i><b>2.1</b> Open the pod bay doors, please, HAL.</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="r-fundamentals.html"><a href="r-fundamentals.html"><i class="fa fa-check"></i><b>3</b> R Fundamentals</a><ul>
<li class="chapter" data-level="3.1" data-path="r-fundamentals.html"><a href="r-fundamentals.html#about-this-chapter"><i class="fa fa-check"></i><b>3.1</b> About this chapter</a></li>
<li class="chapter" data-level="3.2" data-path="r-fundamentals.html"><a href="r-fundamentals.html#working-with-r"><i class="fa fa-check"></i><b>3.2</b> Working with R</a></li>
<li class="chapter" data-level="3.3" data-path="r-fundamentals.html"><a href="r-fundamentals.html#variables"><i class="fa fa-check"></i><b>3.3</b> Variables</a><ul>
<li class="chapter" data-level="3.3.1" data-path="r-fundamentals.html"><a href="r-fundamentals.html#using-objects-and-functions"><i class="fa fa-check"></i><b>3.3.1</b> Using objects and functions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="r-fundamentals.html"><a href="r-fundamentals.html#dataframes"><i class="fa fa-check"></i><b>3.4</b> Dataframes</a></li>
<li class="chapter" data-level="3.5" data-path="r-fundamentals.html"><a href="r-fundamentals.html#packages"><i class="fa fa-check"></i><b>3.5</b> Packages</a></li>
<li class="chapter" data-level="3.6" data-path="r-fundamentals.html"><a href="r-fundamentals.html#using-r-help"><i class="fa fa-check"></i><b>3.6</b> Using R Help</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>4</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="4.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#about-this-chapter-1"><i class="fa fa-check"></i><b>4.1</b> About this chapter</a></li>
<li class="chapter" data-level="4.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#p-features-and-n-cases"><i class="fa fa-check"></i><b>4.2</b> <span class="math inline">\(p\)</span> Features and <span class="math inline">\(n\)</span> Cases</a></li>
<li class="chapter" data-level="4.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering"><i class="fa fa-check"></i><b>4.3</b> Clustering</a><ul>
<li class="chapter" data-level="4.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#the-distance-measure-and-matrix"><i class="fa fa-check"></i><b>4.3.1</b> The distance measure and matrix</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#hierarchical-single-linkage-clustering"><i class="fa fa-check"></i><b>4.4</b> Hierarchical (single linkage) Clustering</a><ul>
<li class="chapter" data-level="4.4.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#hierarchical-clustering-in-base-r"><i class="fa fa-check"></i><b>4.4.1</b> Hierarchical clustering in Base R</a></li>
<li class="chapter" data-level="4.4.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustered-heatmaps"><i class="fa fa-check"></i><b>4.4.2</b> Clustered Heatmaps</a></li>
<li class="chapter" data-level="4.4.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#extra-credit-ggplot-and-clusters"><i class="fa fa-check"></i><b>4.4.3</b> Extra Credit: ggplot and clusters</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>4.5</b> K-Means clustering</a><ul>
<li class="chapter" data-level="4.5.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#figure-of-merit"><i class="fa fa-check"></i><b>4.5.1</b> Figure of Merit</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>5</b> Supervised Learning</a><ul>
<li class="chapter" data-level="5.1" data-path="supervised-learning.html"><a href="supervised-learning.html#about-this-chapter-2"><i class="fa fa-check"></i><b>5.1</b> About this chapter</a></li>
<li class="chapter" data-level="5.2" data-path="supervised-learning.html"><a href="supervised-learning.html#labelled-data"><i class="fa fa-check"></i><b>5.2</b> Labelled Data</a></li>
<li class="chapter" data-level="5.3" data-path="supervised-learning.html"><a href="supervised-learning.html#training-and-testing"><i class="fa fa-check"></i><b>5.3</b> Training and Testing</a><ul>
<li class="chapter" data-level="5.3.1" data-path="supervised-learning.html"><a href="supervised-learning.html#the-training-phase"><i class="fa fa-check"></i><b>5.3.1</b> The Training Phase</a></li>
<li class="chapter" data-level="5.3.2" data-path="supervised-learning.html"><a href="supervised-learning.html#the-testing-phase"><i class="fa fa-check"></i><b>5.3.2</b> The Testing Phase</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="supervised-learning.html"><a href="supervised-learning.html#measuring-accuracy"><i class="fa fa-check"></i><b>5.4</b> Measuring accuracy</a><ul>
<li class="chapter" data-level="5.4.1" data-path="supervised-learning.html"><a href="supervised-learning.html#two-ways-to-be-right-true-positives-and-true-negatives"><i class="fa fa-check"></i><b>5.4.1</b> Two ways to be right: True Positives and True Negatives</a></li>
<li class="chapter" data-level="5.4.2" data-path="supervised-learning.html"><a href="supervised-learning.html#two-ways-to-be-wrong-false-postive-and-false-negatives"><i class="fa fa-check"></i><b>5.4.2</b> Two ways to be wrong: False Postive and False Negatives</a></li>
<li class="chapter" data-level="5.4.3" data-path="supervised-learning.html"><a href="supervised-learning.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>5.4.3</b> Sensitivity and Specificity</a></li>
<li class="chapter" data-level="5.4.4" data-path="supervised-learning.html"><a href="supervised-learning.html#other-measures-of-accuracy"><i class="fa fa-check"></i><b>5.4.4</b> Other measures of accuracy</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="supervised-learning.html"><a href="supervised-learning.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>5.5</b> <span class="math inline">\(k\)</span>-Nearest Neighbours</a><ul>
<li class="chapter" data-level="5.5.1" data-path="supervised-learning.html"><a href="supervised-learning.html#training-and-evaluating-knn"><i class="fa fa-check"></i><b>5.5.1</b> Training and evaluating <span class="math inline">\(k\)</span>NN</a></li>
<li class="chapter" data-level="5.5.2" data-path="supervised-learning.html"><a href="supervised-learning.html#using-a-trained-model"><i class="fa fa-check"></i><b>5.5.2</b> Using a trained model</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forest"><i class="fa fa-check"></i><b>5.6</b> Random Forest</a><ul>
<li class="chapter" data-level="5.6.1" data-path="supervised-learning.html"><a href="supervised-learning.html#building-a-random-forest-model"><i class="fa fa-check"></i><b>5.6.1</b> Building a Random Forest Model</a></li>
<li class="chapter" data-level="5.6.2" data-path="supervised-learning.html"><a href="supervised-learning.html#testing-a-random-forest-model"><i class="fa fa-check"></i><b>5.6.2</b> Testing a Random Forest model</a></li>
<li class="chapter" data-level="5.6.3" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forest-with-categorical-predictors"><i class="fa fa-check"></i><b>5.6.3</b> Random Forest with categorical predictors</a></li>
<li class="chapter" data-level="5.6.4" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forest-regression"><i class="fa fa-check"></i><b>5.6.4</b> Random Forest Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>6</b> Deep Learning</a><ul>
<li class="chapter" data-level="6.1" data-path="deep-learning.html"><a href="deep-learning.html#about-this-chapter-3"><i class="fa fa-check"></i><b>6.1</b> About this chapter</a></li>
<li class="chapter" data-level="6.2" data-path="deep-learning.html"><a href="deep-learning.html#feature-selection-in-deep-learning"><i class="fa fa-check"></i><b>6.2</b> Feature Selection in Deep Learning</a></li>
<li class="chapter" data-level="6.3" data-path="deep-learning.html"><a href="deep-learning.html#cryptic-patterns-in-deep-learning"><i class="fa fa-check"></i><b>6.3</b> Cryptic patterns in Deep Learning</a><ul>
<li class="chapter" data-level="6.3.1" data-path="deep-learning.html"><a href="deep-learning.html#implications-of-cryptic-patterns"><i class="fa fa-check"></i><b>6.3.1</b> Implications of Cryptic Patterns</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="deep-learning.html"><a href="deep-learning.html#neural-networks"><i class="fa fa-check"></i><b>6.4</b> Neural Networks</a><ul>
<li class="chapter" data-level="6.4.1" data-path="deep-learning.html"><a href="deep-learning.html#the-perceptron"><i class="fa fa-check"></i><b>6.4.1</b> The Perceptron</a></li>
<li class="chapter" data-level="6.4.2" data-path="deep-learning.html"><a href="deep-learning.html#the-network"><i class="fa fa-check"></i><b>6.4.2</b> The Network</a></li>
<li class="chapter" data-level="6.4.3" data-path="deep-learning.html"><a href="deep-learning.html#neural-network-structure"><i class="fa fa-check"></i><b>6.4.3</b> Neural Network Structure</a></li>
<li class="chapter" data-level="6.4.4" data-path="deep-learning.html"><a href="deep-learning.html#training-to-find-weights"><i class="fa fa-check"></i><b>6.4.4</b> Training to find weights</a></li>
<li class="chapter" data-level="6.4.5" data-path="deep-learning.html"><a href="deep-learning.html#neural-network-training-phases-are-long-and-involved"><i class="fa fa-check"></i><b>6.4.5</b> Neural network training phases are long and involved</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="deep-learning.html"><a href="deep-learning.html#a-simple-neural-network-in-r"><i class="fa fa-check"></i><b>6.5</b> A simple neural network in R</a><ul>
<li class="chapter" data-level="6.5.1" data-path="deep-learning.html"><a href="deep-learning.html#frog-data"><i class="fa fa-check"></i><b>6.5.1</b> Frog Data</a></li>
<li class="chapter" data-level="6.5.2" data-path="deep-learning.html"><a href="deep-learning.html#training-a-3-hidden-layer-neural-network"><i class="fa fa-check"></i><b>6.5.2</b> Training a 3 hidden layer neural network</a></li>
<li class="chapter" data-level="6.5.3" data-path="deep-learning.html"><a href="deep-learning.html#testing-the-neural-network"><i class="fa fa-check"></i><b>6.5.3</b> Testing the neural network</a></li>
<li class="chapter" data-level="6.5.4" data-path="deep-learning.html"><a href="deep-learning.html#examining-the-structure-of-the-neural-network"><i class="fa fa-check"></i><b>6.5.4</b> Examining the structure of the neural network</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tour of Machine Learning Tools</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="deep-learning" class="section level1">
<h1><span class="header-section-number">Topic 6</span> Deep Learning</h1>
<ol style="list-style-type: decimal">
<li>Questions
<ul>
<li>What is Deep Learning?</li>
<li>How is Deep Learning distinct from classical Machine Learning?</li>
</ul></li>
<li>Objectives
<ul>
<li>Discuss how cryptic patterns can find their own important features in arbitrary data sets</li>
<li>Study the outline of training a Neural Network</li>
<li>Build and test a simple Neural Network</li>
</ul></li>
<li>Key Points
<ul>
<li>Deep Learners find important features and patterns in the data automatically</li>
<li>Neural Networks use optimised weights to learn classifications</li>
<li>Power comes at the expense of interpretability</li>
</ul></li>
</ol>
<div id="about-this-chapter-3" class="section level2">
<h2><span class="header-section-number">6.1</span> About this chapter</h2>
<p>In this chapter we’ll look at the latest advance in Machine Learning, a special set of extremely powerful techniques called Deep Learning. Deep learning methods are the ones that have been used in headline grabbing Artificial Intelligence toold from large scale facial recognition, data mining to influence voters on social media, voice recognition like Siri and Alexa, Netflix’s suggestion algorithm, Google’s advertising algorithm, and in science things like AlphaFold - the protein folding prediction tool and medical image analysis.</p>
<p>These tools and algorithms are a wide family of their own and have properties distinct from the machine learning techniques we’ve looked at so far, principally these types are able to select the most important features themselves and work out what is the most reliable data. They are also much more complicated in practice and much more dense so they become a black box and we are less able to interpret how they are making the decisions they make. This is the trade off we make when using Deep Learning.</p>
</div>
<div id="feature-selection-in-deep-learning" class="section level2">
<h2><span class="header-section-number">6.2</span> Feature Selection in Deep Learning</h2>
<p>In the previous tools we’ve looked at we used a <span class="math inline">\(np\)</span> feature matrix, with <span class="math inline">\(p\)</span> features - a column of data for each thing we measured. Feature selection is really important and can make or break the usefulness of a machine learning tool. If the features we select can’t differentiate between the classes, then the machine learning tool will never be able to make good predictions. Consider what it would be like if we tried to work out a person’s hair colour from their height! Height is an easy thing to measure but does it ever predict a person’s hair?</p>
<p>So we must pick our features carefully if we’re to make use of ML generally, but with Deep Learning the algorithms themselves work out which are the most useful features and also patterns within the features and preferentially use them. This leads to a bit of a kitchen sink approach, we can take all the features we like, pump them into a Deep Learning algorithm and let it decide the best way to use them. A practical upside of this then is that our <span class="math inline">\(np\)</span> feature matrix can become very complicated and we can start to squeeze pretty much anything into the training data. We simply have to be able to encode it as numbers somehow.</p>
</div>
<div id="cryptic-patterns-in-deep-learning" class="section level2">
<h2><span class="header-section-number">6.3</span> Cryptic patterns in Deep Learning</h2>
<p>The ability to automatically select features or patterns to use means that the algorithms can find and use patterns that we don’t specify explicitly and in fact don’t even know about. To understand this, let’s work through a protein sequence based example. Our first issue with biological sequences is the question of how to encode it as numbers. One common way of taking a categoric thing and making it numeric is to use ‘One Hot Encoding’. Which looks like this:</p>
<p><img src="figs/onehot.png" /><!-- --></p>
<p>This encoding represents the protein sequence ‘ACDE’, the columns represent the alphabet of amino acids (in alphabetic order), the rows represent the position of sequence. We add a <code>1</code> at the intersection of the position and amino acid to show the amino acid at each position. Each row therefore has only one row.</p>
<p>Once we have an encoding, patterns will start to appear that the algorithms can use. Consider a protein motif, like RHLR - that would look like this in our one hot encoding.</p>
<p><img src="figs/rhlr.png" /><!-- --></p>
<p>Now wherever the pattern crops up the algorithm will see it. The pattern can become associated with a particular class and used as part of the signal for classification. We didn’t have to say ‘protein has RHLR’, if it’s an important pattern and associated with a class or group then the algorithm will use it.</p>
<div id="implications-of-cryptic-patterns" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Implications of Cryptic Patterns</h3>
<p>Being able to select its own features and patterns means that the Deep Learning methods get a special sort of sensitivity. Considering our protein example again, then lots of properties of the proteins that are reliant on sequence at some level will be detectable and useable in some way by the Deep Learner. Things like physico-chemical sequence properties such as hydrophobicity are reliant on the actual amino acids to exist so they can be captured and used.</p>
<p>An important thing to note is that the patterns have only to be associated, not <em>over-represented</em>, on the whole. The Deep Learners might find a pattern that occurs only a few times in millions of example data, but if it is associated pretty uniquely with just one class or group then it can be used. This stands in contrast to typical methods of pattern finding in bioinformatics, which use majority or statistical over-representation. The patterns often have weight with other patterns and these associations increase the patterns power too.</p>
<p>The ability to find cryptic patterns and make associations is reliant on having a great deal of training data. Deep Learning methods do require lots more data than the ML methods we’ve already looked at and this can be a drawback in practice.</p>
<p>Deep Learning models internal representations become very large and hard to interpret, so that actually understanding what they’re using to classify upon can become impossible. This is a significant trade off for the high power that we can get</p>
</div>
</div>
<div id="neural-networks" class="section level2">
<h2><span class="header-section-number">6.4</span> Neural Networks</h2>
<p>The core of most Deep Learning models and model types is the neural network, let’s run through how that works to gain some insight into how it gets its power.</p>
<div id="the-perceptron" class="section level3">
<h3><span class="header-section-number">6.4.1</span> The Perceptron</h3>
<p>Neural networks are made up of units called Perceptrons, these are mathematical structures inspired by biological neurons. They take multiple inputs, integrate them in some way and produce an output. One that worked on our animal matrix might look like this</p>
<p><img src="figs/percepwts/Slide1.png" /><!-- --></p>
</div>
<div id="the-network" class="section level3">
<h3><span class="header-section-number">6.4.2</span> The Network</h3>
<p>Combining lots of perceptrons results in a neural network and at a basic level might work with our animal data like this,</p>
<p><img src="figs/percepwts/Slide2.png" /><!-- --></p>
<p>Making sense of all the integrations from the neural network, that is the calling of a class (in this case <code>is_a_cat</code>) is done by a decision function, here that may look like this</p>
<pre><code>if S &gt; 2,
    cat = 1
else 
    cat = 0</code></pre>
<p>Applied to the animal neural network the animal data classifications end up like this</p>
<p><img src="figs/percepwts/Slide3.png" /><!-- --></p>
<p>Only one of the actual cats was correctly labelled. The clever part of the neural network is to apply weights to each of the features that modify the value they add to the neural network (in the figure below as blue values).</p>
<p><img src="figs/percepwts/Slide4.png" /><!-- --></p>
<p>Weights work to give the more useful features higher values (like <code>meows</code>) and less useful features lower values (like <code>four legs</code>). The network can now more accurately classify the animals in the picture.</p>
</div>
<div id="neural-network-structure" class="section level3">
<h3><span class="header-section-number">6.4.3</span> Neural Network Structure</h3>
<p>The networks needn’t be restricted to simple structures in which the initial inputs go straight to the output, many layers of neurons can be made, each arbitrary numbers of neurons deep. These extra layers are called the hidden layers</p>
<p><img src="figs/layers.png" /><!-- --></p>
<p>The hidden layers increase the power of the neural network by allowing for further integration of information and extra weighting. But they also make the network more obscure and hard to read, again this is how the power of the neural network comes at the expense of interpretability.</p>
</div>
<div id="training-to-find-weights" class="section level3">
<h3><span class="header-section-number">6.4.4</span> Training to find weights</h3>
<p>The main part of training the neural network and the place it makes itself powerful is in the weight finding phase. This is called the learning or training phase. To do this the training algorithm goes back and forth across the network methodically adjusting the weights until it sees no further improvement when classifying on the training data - it is constantly comparing its current state against the answers in the training data.</p>
</div>
<div id="neural-network-training-phases-are-long-and-involved" class="section level3">
<h3><span class="header-section-number">6.4.5</span> Neural network training phases are long and involved</h3>
<p>As you can see there is a lot about neural networks to be specified and optimised. The number and depth of hidden layers that is optimal varies for each data set and there is no rule to follow as to what will be best. It is also not true that bigger is always better. The weights of the neural network must also be optimised for every data set, and we must be careful to use training data that is distinct from our test data to be confident in the generality of our resulting model. As a result of these considerations the training and testing phases of neural networks are particularly involved.</p>
<p>We won’t go through that whole procedure here, though you should be aware of it as it is the key to a truly useful deep learning model. But we will try out a small neural network in R.</p>
</div>
</div>
<div id="a-simple-neural-network-in-r" class="section level2">
<h2><span class="header-section-number">6.5</span> A simple neural network in R</h2>
<div id="frog-data" class="section level3">
<h3><span class="header-section-number">6.5.1</span> Frog Data</h3>
<p>In this example we shall use some data on amphibian presence at various sites. Here’s a <code>glimpse()</code> of the <code>train_rows</code>, we also have a <code>test_rows</code></p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="deep-learning.html#cb85-1"></a>dplyr<span class="op">::</span><span class="kw">glimpse</span>(train_rows)</span></code></pre></div>
<pre><code>## Rows: 94
## Columns: 20
## $ SR                 &lt;dbl&gt; 1000, 100, 200, 30000, 10050, 700, 50, 8000, 2500, …
## $ NR                 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 1, 1, 1, …
## $ TR                 &lt;dbl&gt; 1, 1, 5, 1, 1, 5, 1, 1, 1, 2, 1, 1, 1, 1, 14, 1, 14…
## $ VR                 &lt;dbl&gt; 3, 2, 1, 3, 2, 2, 2, 3, 3, 0, 1, 2, 3, 2, 3, 3, 1, …
## $ SUR1               &lt;dbl&gt; 2, 2, 10, 1, 1, 10, 2, 2, 10, 6, 2, 2, 2, 2, 7, 2, …
## $ SUR2               &lt;dbl&gt; 1, 7, 6, 1, 10, 6, 7, 10, 2, 9, 7, 7, 2, 10, 2, 2, …
## $ SUR3               &lt;dbl&gt; 9, 6, 10, 1, 6, 9, 10, 7, 6, 2, 6, 9, 1, 10, 1, 7, …
## $ UR                 &lt;dbl&gt; 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, …
## $ FR                 &lt;dbl&gt; 0, 0, 4, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, …
## $ OR                 &lt;dbl&gt; 100, 100, 75, 100, 100, 100, 100, 100, 100, 50, 100…
## $ RR                 &lt;dbl&gt; 2, 2, 1, 2, 5, 1, 5, 9, 0, 0, 0, 0, 5, 1, 5, 0, 5, …
## $ BR                 &lt;dbl&gt; 5, 2, 1, 10, 5, 1, 5, 9, 1, 0, 0, 0, 5, 5, 5, 1, 5,…
## $ MR                 &lt;dbl&gt; 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ CR                 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
## $ Green_frogs        &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, …
## $ Brown_frogs        &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, …
## $ Common_toad        &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, …
## $ Tree_frog          &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, …
## $ Common_newt        &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, …
## $ Great_crested_newt &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …</code></pre>
<p>These data are from <span class="citation">(Blachnik, Sołtysiak, and Dąbrowska <a href="#ref-frogs" role="doc-biblioref">2019</a>)</span> originally and you can see a description of the 20 columns at <a href="https://archive.ics.uci.edu/ml/datasets/Amphibians">https://archive.ics.uci.edu/ml/datasets/Amphibians</a>. Briefly, they are things like the presence and size and maintenance of reservoirs and the surrounding area, whether humans use the area, whether there is fishing. All potentially pertinent measurements. The presence of different types of amphibian species are recorded as 1 for present, and 0 for not present.</p>
<p>We could use a neural network to predict any of the species listed, but let’s work on predicting <code>Green_frogs</code>.</p>
<p>We’ll use the straightforward <code>neuralnet()</code> function in the <code>neuralnet</code> package for this. It can take an R formula specification, which as you’ll recall takes the form <code>y ~ feature_1 + feature_2 ...</code> where <code>y</code> is the thing to be predicted and <code>feature_x</code> are the features to input for prediction with. With 19 to enter, that’s lots of typing, so I’ve squashed it into a variable called <code>long_formula</code></p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="deep-learning.html#cb87-1"></a>long_formula</span></code></pre></div>
<pre><code>## Green_frogs ~ SR + NR + TR + VR + SUR1 + SUR2 + SUR3 + UR + FR + 
##     OR + RR + BR + MR + CR + Green_frogs + Brown_frogs + Common_toad + 
##     Tree_frog + Common_newt + Great_crested_newt</code></pre>
</div>
<div id="training-a-3-hidden-layer-neural-network" class="section level3">
<h3><span class="header-section-number">6.5.2</span> Training a 3 hidden layer neural network</h3>
<p>We can put the formula into the function <code>neuralnet()</code> specify the training data and the depth of the hidden layers. Here we’ll have 3, with 15, 10 and 5 neurons respectively.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="deep-learning.html#cb89-1"></a><span class="kw">library</span>(neuralnet)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;neuralnet&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     compute</code></pre>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="deep-learning.html#cb92-1"></a>nn &lt;-<span class="st"> </span><span class="kw">neuralnet</span>(long_formula, train_rows, <span class="dt">hidden=</span><span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">10</span>,<span class="dv">5</span>), <span class="dt">linear.output=</span><span class="ot">FALSE</span>)</span></code></pre></div>
<p>That single step builds the neural network, and trains it and gives it back to use so we can use it to make predictions with. Of course the first thing we want to make predictions on is our test set so we can evaluate the accuracy.</p>
</div>
<div id="testing-the-neural-network" class="section level3">
<h3><span class="header-section-number">6.5.3</span> Testing the neural network</h3>
<p>The <code>compute()</code> function takes a neural network model and data and creates predictions. Here we feed it our test data. However when we look at the resulting predictions (stored in the <code>net.result</code> slot in our <code>predictions</code> object) we see something odd.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="deep-learning.html#cb93-1"></a>predictions &lt;-<span class="st"> </span><span class="kw">compute</span>(nn, test_rows)</span>
<span id="cb93-2"><a href="deep-learning.html#cb93-2"></a><span class="kw">head</span>(predictions<span class="op">$</span>net.result)</span></code></pre></div>
<pre><code>##             [,1]
## [1,] 0.998154554
## [2,] 0.998147802
## [3,] 0.008503821
## [4,] 0.998154554
## [5,] 0.996971835
## [6,] 0.006267082</code></pre>
<p>The predictions are not of classes, but are actually numbers that represent the level of sureness the model has that the site has Green Frogs. This value is sometimes useful, but we need to convert it to classes to evaluate it. As the values run between 0 and 1 we can do that by simple rounding so that any prediction over 0.5 is considered a present prediction, anything below is consider an absent predictions (other algorithms and functions exist for this conversion).</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="deep-learning.html#cb95-1"></a>binary_predictions &lt;-<span class="st"> </span><span class="kw">round</span>(predictions<span class="op">$</span>net.result,<span class="dt">digits=</span><span class="dv">0</span>)</span></code></pre></div>
<p>We can then put those binarised predictions into the <code>confusionMatrix()</code> function we used previously alongside the true values from the <code>test_rows</code> data (remembering to convert them to <code>factors</code> as they are not already).</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="deep-learning.html#cb96-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb96-2"><a href="deep-learning.html#cb96-2"></a><span class="kw">confusionMatrix</span>(<span class="kw">factor</span>(test_rows<span class="op">$</span>Green_frogs), <span class="kw">factor</span>(binary_predictions))</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  0  1
##          0 30  4
##          1 19 42
##                                           
##                Accuracy : 0.7579          
##                  95% CI : (0.6592, 0.8399)
##     No Information Rate : 0.5158          
##     P-Value [Acc &gt; NIR] : 1.097e-06       
##                                           
##                   Kappa : 0.5201          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.003509        
##                                           
##             Sensitivity : 0.6122          
##             Specificity : 0.9130          
##          Pos Pred Value : 0.8824          
##          Neg Pred Value : 0.6885          
##              Prevalence : 0.5158          
##          Detection Rate : 0.3158          
##    Detection Prevalence : 0.3579          
##       Balanced Accuracy : 0.7626          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p>We can see the resulting network has about 60% sensitivity and 90% specificity, so missing a lot of real green frog sites.</p>
<p>As you can imagine the exact choice of the parameters can make a difference on final neural network performance. This is just one instance. In real analyses we would try out many different hidden layer and other parameter configurations and select the best performing at the the testing stage. That may then even move on to a further fine tuning stage, the development of machine learners is art as much as it is science.</p>
</div>
<div id="examining-the-structure-of-the-neural-network" class="section level3">
<h3><span class="header-section-number">6.5.4</span> Examining the structure of the neural network</h3>
<p>The <code>neuralnet</code> package we used here was chosen not least because it is straightforward and fast, but also because it is possible to get a plot of the created network.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="deep-learning.html#cb98-1"></a><span class="kw">plot</span>(nn)</span></code></pre></div>
<p><img src="figs/nn_viz.png" /><!-- --></p>
<p>Note that the first layer corresponds to the input columns in the data with one neuron each, these then feed into the 3 hidden layers we specified of 20,15 and 5 layers each and finally the one neuron layer intergrating everything to give us the final prediction on whether we have a Green Frog.</p>
<p>We can see that the neural network we made is really complex. Even with just the small number of input features and hidden layers we have the combinations of weights and their effect into the next layer is too hard to understand (even if the plot were readable). This shows us how neural network structures become black boxes, we can’t be sure which of the input variables (or combinations of which) were most important in making the classifications.</p>

<div class="roundup">
<ul>
<li>Deep Learners choose their own features</li>
<li>Deep Learners like neural networks can work on patterns we dont explicitly state</li>
<li>Neural network training means finding weights that give the best classifications</li>
<li>Neural networks are black boxes and hard to interpret</li>
</ul>
</div>


</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-frogs">
<p>Blachnik, Marcin, Marek Sołtysiak, and Dominika Dąbrowska. 2019. “Predicting Presence of Amphibian Species Using Features Obtained from Gis and Satellite Images.” <em>ISPRS International Journal of Geo-Information</em> 8 (3). <a href="https://doi.org/10.3390/ijgi8030123">https://doi.org/10.3390/ijgi8030123</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="supervised-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/05-deep_learning.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["machine_learning.pdf", "machine_learning.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
