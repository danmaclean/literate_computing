<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Topic 5 Supervised Learning | A Tour of Machine Learning Tools</title>
  <meta name="description" content="A tour of selected machine learning tools that are used in plant pathogenomics research" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Topic 5 Supervised Learning | A Tour of Machine Learning Tools" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A tour of selected machine learning tools that are used in plant pathogenomics research" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Topic 5 Supervised Learning | A Tour of Machine Learning Tools" />
  
  <meta name="twitter:description" content="A tour of selected machine learning tools that are used in plant pathogenomics research" />
  

<meta name="author" content="Dan MacLean" />


<meta name="date" content="2021-04-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="unsupervised-learning.html"/>
<link rel="next" href="deep-learning.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intro to ML</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Setting up</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a><ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#knowledge-prerequisites"><i class="fa fa-check"></i><b>1.1.1</b> Knowledge prerequisites</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#software-prerequisites"><i class="fa fa-check"></i><b>1.1.2</b> Software prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#installing-r"><i class="fa fa-check"></i><b>1.2</b> Installing R</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#installing-rstudio"><i class="fa fa-check"></i><b>1.3</b> Installing RStudio</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#installing-r-packages-in-rstudio"><i class="fa fa-check"></i><b>1.4</b> Installing R packages in RStudio</a><ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#standard-packages"><i class="fa fa-check"></i><b>1.4.1</b> Standard packages</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="motivation.html"><a href="motivation.html"><i class="fa fa-check"></i><b>2</b> Motivation</a><ul>
<li class="chapter" data-level="2.1" data-path="motivation.html"><a href="motivation.html#open-the-pod-bay-doors-please-hal."><i class="fa fa-check"></i><b>2.1</b> Open the pod bay doors, please, HAL.</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="r-fundamentals.html"><a href="r-fundamentals.html"><i class="fa fa-check"></i><b>3</b> R Fundamentals</a><ul>
<li class="chapter" data-level="3.1" data-path="r-fundamentals.html"><a href="r-fundamentals.html#about-this-chapter"><i class="fa fa-check"></i><b>3.1</b> About this chapter</a></li>
<li class="chapter" data-level="3.2" data-path="r-fundamentals.html"><a href="r-fundamentals.html#working-with-r"><i class="fa fa-check"></i><b>3.2</b> Working with R</a></li>
<li class="chapter" data-level="3.3" data-path="r-fundamentals.html"><a href="r-fundamentals.html#variables"><i class="fa fa-check"></i><b>3.3</b> Variables</a><ul>
<li class="chapter" data-level="3.3.1" data-path="r-fundamentals.html"><a href="r-fundamentals.html#using-objects-and-functions"><i class="fa fa-check"></i><b>3.3.1</b> Using objects and functions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="r-fundamentals.html"><a href="r-fundamentals.html#dataframes"><i class="fa fa-check"></i><b>3.4</b> Dataframes</a></li>
<li class="chapter" data-level="3.5" data-path="r-fundamentals.html"><a href="r-fundamentals.html#packages"><i class="fa fa-check"></i><b>3.5</b> Packages</a></li>
<li class="chapter" data-level="3.6" data-path="r-fundamentals.html"><a href="r-fundamentals.html#using-r-help"><i class="fa fa-check"></i><b>3.6</b> Using R Help</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>4</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="4.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#about-this-chapter-1"><i class="fa fa-check"></i><b>4.1</b> About this chapter</a></li>
<li class="chapter" data-level="4.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#p-features-and-n-cases"><i class="fa fa-check"></i><b>4.2</b> <span class="math inline">\(p\)</span> Features and <span class="math inline">\(n\)</span> Cases</a></li>
<li class="chapter" data-level="4.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering"><i class="fa fa-check"></i><b>4.3</b> Clustering</a><ul>
<li class="chapter" data-level="4.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#the-distance-measure-and-matrix"><i class="fa fa-check"></i><b>4.3.1</b> The distance measure and matrix</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#hierarchical-single-linkage-clustering"><i class="fa fa-check"></i><b>4.4</b> Hierarchical (single linkage) Clustering</a><ul>
<li class="chapter" data-level="4.4.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#hierarchical-clustering-in-base-r"><i class="fa fa-check"></i><b>4.4.1</b> Hierarchical clustering in Base R</a></li>
<li class="chapter" data-level="4.4.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustered-heatmaps"><i class="fa fa-check"></i><b>4.4.2</b> Clustered Heatmaps</a></li>
<li class="chapter" data-level="4.4.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#extra-credit-ggplot-and-clusters"><i class="fa fa-check"></i><b>4.4.3</b> Extra Credit: ggplot and clusters</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>4.5</b> K-Means clustering</a><ul>
<li class="chapter" data-level="4.5.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#figure-of-merit"><i class="fa fa-check"></i><b>4.5.1</b> Figure of Merit</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>5</b> Supervised Learning</a><ul>
<li class="chapter" data-level="5.1" data-path="supervised-learning.html"><a href="supervised-learning.html#about-this-chapter-2"><i class="fa fa-check"></i><b>5.1</b> About this chapter</a></li>
<li class="chapter" data-level="5.2" data-path="supervised-learning.html"><a href="supervised-learning.html#labelled-data"><i class="fa fa-check"></i><b>5.2</b> Labelled Data</a></li>
<li class="chapter" data-level="5.3" data-path="supervised-learning.html"><a href="supervised-learning.html#training-and-testing"><i class="fa fa-check"></i><b>5.3</b> Training and Testing</a><ul>
<li class="chapter" data-level="5.3.1" data-path="supervised-learning.html"><a href="supervised-learning.html#the-training-phase"><i class="fa fa-check"></i><b>5.3.1</b> The Training Phase</a></li>
<li class="chapter" data-level="5.3.2" data-path="supervised-learning.html"><a href="supervised-learning.html#the-testing-phase"><i class="fa fa-check"></i><b>5.3.2</b> The Testing Phase</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="supervised-learning.html"><a href="supervised-learning.html#measuring-accuracy"><i class="fa fa-check"></i><b>5.4</b> Measuring accuracy</a><ul>
<li class="chapter" data-level="5.4.1" data-path="supervised-learning.html"><a href="supervised-learning.html#two-ways-to-be-right-true-positives-and-true-negatives"><i class="fa fa-check"></i><b>5.4.1</b> Two ways to be right: True Positives and True Negatives</a></li>
<li class="chapter" data-level="5.4.2" data-path="supervised-learning.html"><a href="supervised-learning.html#two-ways-to-be-wrong-false-postive-and-false-negatives"><i class="fa fa-check"></i><b>5.4.2</b> Two ways to be wrong: False Postive and False Negatives</a></li>
<li class="chapter" data-level="5.4.3" data-path="supervised-learning.html"><a href="supervised-learning.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>5.4.3</b> Sensitivity and Specificity</a></li>
<li class="chapter" data-level="5.4.4" data-path="supervised-learning.html"><a href="supervised-learning.html#other-measures-of-accuracy"><i class="fa fa-check"></i><b>5.4.4</b> Other measures of accuracy</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="supervised-learning.html"><a href="supervised-learning.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>5.5</b> <span class="math inline">\(k\)</span>-Nearest Neighbours</a><ul>
<li class="chapter" data-level="5.5.1" data-path="supervised-learning.html"><a href="supervised-learning.html#training-and-evaluating-knn"><i class="fa fa-check"></i><b>5.5.1</b> Training and evaluating <span class="math inline">\(k\)</span>NN</a></li>
<li class="chapter" data-level="5.5.2" data-path="supervised-learning.html"><a href="supervised-learning.html#using-a-trained-model"><i class="fa fa-check"></i><b>5.5.2</b> Using a trained model</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forest"><i class="fa fa-check"></i><b>5.6</b> Random Forest</a><ul>
<li class="chapter" data-level="5.6.1" data-path="supervised-learning.html"><a href="supervised-learning.html#building-a-random-forest-model"><i class="fa fa-check"></i><b>5.6.1</b> Building a Random Forest Model</a></li>
<li class="chapter" data-level="5.6.2" data-path="supervised-learning.html"><a href="supervised-learning.html#testing-a-random-forest-model"><i class="fa fa-check"></i><b>5.6.2</b> Testing a Random Forest model</a></li>
<li class="chapter" data-level="5.6.3" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forest-with-categorical-predictors"><i class="fa fa-check"></i><b>5.6.3</b> Random Forest with categorical predictors</a></li>
<li class="chapter" data-level="5.6.4" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forest-regression"><i class="fa fa-check"></i><b>5.6.4</b> Random Forest Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>6</b> Deep Learning</a><ul>
<li class="chapter" data-level="6.1" data-path="deep-learning.html"><a href="deep-learning.html#about-this-chapter-3"><i class="fa fa-check"></i><b>6.1</b> About this chapter</a></li>
<li class="chapter" data-level="6.2" data-path="deep-learning.html"><a href="deep-learning.html#feature-selection-in-deep-learning"><i class="fa fa-check"></i><b>6.2</b> Feature Selection in Deep Learning</a></li>
<li class="chapter" data-level="6.3" data-path="deep-learning.html"><a href="deep-learning.html#cryptic-patterns-in-deep-learning"><i class="fa fa-check"></i><b>6.3</b> Cryptic patterns in Deep Learning</a><ul>
<li class="chapter" data-level="6.3.1" data-path="deep-learning.html"><a href="deep-learning.html#implications-of-cryptic-patterns"><i class="fa fa-check"></i><b>6.3.1</b> Implications of Cryptic Patterns</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="deep-learning.html"><a href="deep-learning.html#neural-networks"><i class="fa fa-check"></i><b>6.4</b> Neural Networks</a><ul>
<li class="chapter" data-level="6.4.1" data-path="deep-learning.html"><a href="deep-learning.html#the-perceptron"><i class="fa fa-check"></i><b>6.4.1</b> The Perceptron</a></li>
<li class="chapter" data-level="6.4.2" data-path="deep-learning.html"><a href="deep-learning.html#the-network"><i class="fa fa-check"></i><b>6.4.2</b> The Network</a></li>
<li class="chapter" data-level="6.4.3" data-path="deep-learning.html"><a href="deep-learning.html#neural-network-structure"><i class="fa fa-check"></i><b>6.4.3</b> Neural Network Structure</a></li>
<li class="chapter" data-level="6.4.4" data-path="deep-learning.html"><a href="deep-learning.html#training-to-find-weights"><i class="fa fa-check"></i><b>6.4.4</b> Training to find weights</a></li>
<li class="chapter" data-level="6.4.5" data-path="deep-learning.html"><a href="deep-learning.html#neural-network-training-phases-are-long-and-involved"><i class="fa fa-check"></i><b>6.4.5</b> Neural network training phases are long and involved</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="deep-learning.html"><a href="deep-learning.html#a-simple-neural-network-in-r"><i class="fa fa-check"></i><b>6.5</b> A simple neural network in R</a><ul>
<li class="chapter" data-level="6.5.1" data-path="deep-learning.html"><a href="deep-learning.html#frog-data"><i class="fa fa-check"></i><b>6.5.1</b> Frog Data</a></li>
<li class="chapter" data-level="6.5.2" data-path="deep-learning.html"><a href="deep-learning.html#training-a-3-hidden-layer-neural-network"><i class="fa fa-check"></i><b>6.5.2</b> Training a 3 hidden layer neural network</a></li>
<li class="chapter" data-level="6.5.3" data-path="deep-learning.html"><a href="deep-learning.html#testing-the-neural-network"><i class="fa fa-check"></i><b>6.5.3</b> Testing the neural network</a></li>
<li class="chapter" data-level="6.5.4" data-path="deep-learning.html"><a href="deep-learning.html#examining-the-structure-of-the-neural-network"><i class="fa fa-check"></i><b>6.5.4</b> Examining the structure of the neural network</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tour of Machine Learning Tools</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="supervised-learning" class="section level1">
<h1><span class="header-section-number">Topic 5</span> Supervised Learning</h1>
<div id="about-this-chapter-2" class="section level2">
<h2><span class="header-section-number">5.1</span> About this chapter</h2>
<ol style="list-style-type: decimal">
<li>Questions
<ul>
<li>How can I find items in data that are like things I already know about?</li>
</ul></li>
<li>Objectives
<ul>
<li>Understand labelled data and classification</li>
<li>Understand training and test data</li>
<li>Understand K nearest neighbours and Random Forest</li>
</ul></li>
<li>Key Points
<ul>
<li>Supervised learning is classifying cases or elements based on examples that we already know</li>
<li>Good training data is key</li>
<li>Don’t mix test and training data</li>
</ul></li>
</ol>
<p>In this chapter we’ll take a look at supervised learning tools. It’s called supervised learning because we have a set of data that we have already classified into one or more groups and the algorithms use that as guide and try to fit some other unknown data into the groups we’ve specified, so the classification is supervised in the sense that there are known examples of the groups. Again the input data is usually a data matrix of some features, like measurements or gene expression values.</p>
</div>
<div id="labelled-data" class="section level2">
<h2><span class="header-section-number">5.2</span> Labelled Data</h2>
<p>For supervised learning algorithms we need to give examples of our categories. This is called labelling the data. And in most cases we can achieve this just by extending our <span class="math inline">\(np\)</span> features/cases data matrix by one column and add a label in there, usually as a number. For our animal matrix example that would look like this if we wanted to label our data as a cat or not.</p>
<p><img src="figs/labels.png" /><!-- --></p>
<p>The object for the learning algorithm is then to guess labels for data that we don’t know beforehand. So in our animal matrix example, that looks like this</p>
<p><img src="figs/labels2.png" /><!-- --></p>
</div>
<div id="training-and-testing" class="section level2">
<h2><span class="header-section-number">5.3</span> Training and Testing</h2>
<div id="the-training-phase" class="section level3">
<h3><span class="header-section-number">5.3.1</span> The Training Phase</h3>
<p>Most supervised learning algorithms have an initial training phase. Training is a part of the procedure where the algorithm creates a model - an internal representation of the data and the associated categories or groups - that it can later use to tell which of our categories a new observation or case belong to. Each type of supervised learner has a different approach to training.</p>
</div>
<div id="the-testing-phase" class="section level3">
<h3><span class="header-section-number">5.3.2</span> The Testing Phase</h3>
<p>Once we have a trained model we must evaluate its accuracy. If we can’t tell how accurate the model is, then we can’t trust it’s predictions and there is not point proceeding. We can test the model on data that <em>we</em> know the labels of but that the model <em>hasn’t seen before</em>. The testing phase is crucial and it is imperative that we don’t use the same data for testing that we used for training, doing so would be like giving a student the answers before the test. The accuracy would be artificially high as they’d already seen the right answers. Once we have a good test of the model done we can use it. Ideally, we’d want the model to give high accuracy, but that can be subjective. For some applications we might need 99% or greater accuracy, in others just getting an answer better than random would do.</p>
</div>
</div>
<div id="measuring-accuracy" class="section level2">
<h2><span class="header-section-number">5.4</span> Measuring accuracy</h2>
<p>Measuring accuracy of a model in the testing phase is less straightforward than we might first think We might assume that all we have to do is count the number of test cases that we got correct, but that is only one quarter of the story at best!. In fact, for a binary classification (a model that knows only two groups, e.g in our animal example a model that can say whether it thinks something is or isn’t a cat) there are two ways to be right and two ways to be wrong and we must calculate as many of these we can in order to get a good accuracy estimate. For a model with more than two groups or for models trying to predict a quantity rather than a group the question is more complicated and we’ll look at those later.</p>
<div id="two-ways-to-be-right-true-positives-and-true-negatives" class="section level3">
<h3><span class="header-section-number">5.4.1</span> Two ways to be right: True Positives and True Negatives</h3>
<p>The two ways to be right are to get a correct positive classification - a True Positive and a correct negative classification - a True Negative. These are easier to understand graphically. In the figure below we have a set of trained model generated answers and their true classes.</p>
<p><img src="figs/tptn.png" /><!-- --></p>
<p>A True Positive occurs when the model classifies a case positively (is a cat) and is correct, similarly a True Negative occurs when the model classifies a case negatively (is not a cat) and is correct.</p>
</div>
<div id="two-ways-to-be-wrong-false-postive-and-false-negatives" class="section level3">
<h3><span class="header-section-number">5.4.2</span> Two ways to be wrong: False Postive and False Negatives</h3>
<p>The two ways to be wrong are False Positive and false negative classifications False Positives and False Negatives. A False Positive occurs when the model classifies a non-cat as a cat and a False Negative occurs when the model classifies a cat as not a cat.</p>
<p><img src="figs/fpfn.png" /><!-- --></p>
</div>
<div id="sensitivity-and-specificity" class="section level3">
<h3><span class="header-section-number">5.4.3</span> Sensitivity and Specificity</h3>
<p>For a given set of test data for which we know the true labels, we run the model and get it’s classifications. We can count up the True/False Positive/Negatives and calculate two quantities Sensitivity and Specificity. Sensitivity tells us roughly what proportion of True Positives we got, given the errors and Specificity tells how few wrong calls we made. The two measures are therefore complementary and are used together to get a picture of how well the model performs. A good model is high in both. The quantities are calculated as follows</p>
<p><span class="math inline">\(Sensitivity = \frac{TP}{TP+FN}\)</span>
<span class="math inline">\(Specificity = \frac{TN}{TN+FP}\)</span></p>
</div>
<div id="other-measures-of-accuracy" class="section level3">
<h3><span class="header-section-number">5.4.4</span> Other measures of accuracy</h3>
<p>There are in fact, many other measures of accuracy in use beyond sensitivity and specificity. These include things called <span class="math inline">\(F\)</span> scores, precision and recall, FDRs and (confusingly) one actually called accuracy. It’s important to know that they are all a bit different and give different measures but they all try to capture the ‘rightness’ or ‘accuracy’ of our classifiers. As we try out different tools we will see other measures.</p>
</div>
</div>
<div id="k-nearest-neighbours" class="section level2">
<h2><span class="header-section-number">5.5</span> <span class="math inline">\(k\)</span>-Nearest Neighbours</h2>
<p>The <span class="math inline">\(k\)</span>-Nearest Neighbour algorithm is a multi-class capable classification algorithm. Like the unsupervised methods this relies on distance measures between cases/elements and tries to apply a class to an unknown element by looking at the number of nearest neighbours classes. Roughly, the unknown case gets the class of the majority of the <span class="math inline">\(k\)</span> nearest neighbours. We can see an example in the figure below</p>
<p><img src="figs/knn.png" /><!-- --></p>
<p>If we set <span class="math inline">\(k\)</span> to be 5 then Unknown case A has 3 orange squares and 2 red circles in its 5 nearest neighbours, so unknown case A would be classified as an orange square. Similarly, unknown case B has more green triangles in its <span class="math inline">\(k\)</span> nearest neighbours so it gets classified as a green triangle. Note how the known class labels are crucial in putting the unknown cases into classes. This approach only works because we have some known examples. Also note how much harder the algorithm would find the task if there were too few examples of each class. For this and many other types of supervised learning algorithm, the more training data we have, the better.</p>
<div id="training-and-evaluating-knn" class="section level3">
<h3><span class="header-section-number">5.5.1</span> Training and evaluating <span class="math inline">\(k\)</span>NN</h3>
<p>Let’s run through using the algorithm with the data below. The first phase is training and evaluation. There are 3 sets we will use, a training set of 55 points, which is labelled in a separate vector (<code>train_data</code> and <code>train_labels</code>), a test set of 20 points that is labelled (<code>test_data</code> and <code>test_labels</code>) and an unlabelled, unknown data set of 75 points that we wish to label using <span class="math inline">\(k\)</span> Nearest Neighbours.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="supervised-learning.html#cb46-1"></a>dplyr<span class="op">::</span><span class="kw">glimpse</span>(train_data)</span></code></pre></div>
<pre><code>## Rows: 55
## Columns: 4
## $ measure1 &lt;dbl&gt; 6.7, 5.4, 6.4, 5.0, 5.3, 5.4, 5.4, 5.7, 6.0, 5.1, 6.0, 5.8, 4…
## $ measure2 &lt;dbl&gt; 3.1, 3.4, 3.2, 3.2, 3.7, 3.9, 3.0, 4.4, 2.9, 3.8, 2.2, 2.6, 3…
## $ measure3 &lt;dbl&gt; 5.6, 1.7, 5.3, 1.2, 1.5, 1.7, 4.5, 1.5, 4.5, 1.9, 4.0, 4.0, 1…
## $ measure4 &lt;dbl&gt; 2.4, 0.2, 2.3, 0.2, 0.2, 0.4, 1.5, 0.4, 1.5, 0.4, 1.0, 1.2, 0…</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="supervised-learning.html#cb48-1"></a>train_labels</span></code></pre></div>
<pre><code>##  [1] C A C A A A B A B A B B A A B C C A A B A B B C A A A A C C B C B A B C C B
## [39] A A B C B C C A C B A A A B B C B
## Levels: A B C</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="supervised-learning.html#cb50-1"></a>dplyr<span class="op">::</span><span class="kw">glimpse</span>(test_data)</span></code></pre></div>
<pre><code>## Rows: 20
## Columns: 4
## $ measure1 &lt;dbl&gt; 5.6, 6.7, 6.3, 6.3, 5.0, 7.2, 6.2, 6.7, 4.6, 5.1, 6.0, 6.7, 7…
## $ measure2 &lt;dbl&gt; 3.0, 2.5, 3.3, 2.7, 2.3, 3.6, 2.2, 3.1, 3.4, 3.8, 2.2, 3.3, 3…
## $ measure3 &lt;dbl&gt; 4.1, 5.8, 6.0, 4.9, 3.3, 6.1, 4.5, 4.7, 1.4, 1.6, 5.0, 5.7, 6…
## $ measure4 &lt;dbl&gt; 1.3, 1.8, 2.5, 1.8, 1.0, 2.5, 1.5, 1.5, 0.3, 0.2, 1.5, 2.1, 2…</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="supervised-learning.html#cb52-1"></a>test_labels</span></code></pre></div>
<pre><code>##  [1] B C C C B C B B A A C C C A A B A A B A
## Levels: A B C</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="supervised-learning.html#cb54-1"></a>dplyr<span class="op">::</span><span class="kw">glimpse</span>(unknown_data)</span></code></pre></div>
<pre><code>## Rows: 75
## Columns: 4
## $ measure1 &lt;dbl&gt; 5.6, 5.0, 6.3, 6.1, 5.8, 5.5, 5.1, 5.1, 7.2, 5.0, 4.7, 7.7, 5…
## $ measure2 &lt;dbl&gt; 2.9, 3.6, 2.8, 2.8, 2.7, 2.6, 3.8, 3.7, 3.0, 3.0, 3.2, 2.8, 3…
## $ measure3 &lt;dbl&gt; 3.6, 1.4, 5.1, 4.7, 5.1, 4.4, 1.5, 1.5, 5.8, 1.6, 1.6, 6.7, 1…
## $ measure4 &lt;dbl&gt; 1.3, 0.2, 1.5, 1.2, 1.9, 1.2, 0.3, 0.4, 1.6, 0.2, 0.2, 2.0, 0…</code></pre>
<p>The first step is to train and test a model. As we are going to go through the process twice (one evaluating, one with unknown data), we must remember to control the random element of the algorithm. <code>set.seed()</code> with a consistent argument (<code>123</code>) puts the random number generator back to the same place each time allowing reproducibility.</p>
<p>The <code>knn()</code> function is in the <code>class</code> package so we load that and pass it the <code>train_data</code> to learn from and the known <code>test_data</code> to predict groups on. The <code>cl</code> parameter gets the vector of <code>train_labels</code>. Finally the <span class="math inline">\(k\)</span> nearest neighbours is passed as <code>k</code>, here <code>9</code>.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="supervised-learning.html#cb56-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb56-2"><a href="supervised-learning.html#cb56-2"></a><span class="kw">library</span>(class)</span>
<span id="cb56-3"><a href="supervised-learning.html#cb56-3"></a>test_set_predictions &lt;-<span class="st"> </span><span class="kw">knn</span>(train_data, <span class="dt">test=</span>test_data, <span class="dt">cl =</span> train_labels, <span class="dt">k=</span><span class="dv">9</span>)</span>
<span id="cb56-4"><a href="supervised-learning.html#cb56-4"></a>test_set_predictions</span></code></pre></div>
<pre><code>##  [1] B C C C B C B B A A B C C A A B A A B A
## Levels: A B C</code></pre>
<p>As we can see, the predictions are returned as vector whose elements correspond to the rows of <code>test_data</code>. We can check the accuracy of the predictions by comparing the predictions with the known labels. The <code>caret</code> package function <code>confusionMatrix()</code> returns an object with lots of useful information.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="supervised-learning.html#cb58-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb58-2"><a href="supervised-learning.html#cb58-2"></a><span class="kw">confusionMatrix</span>(test_set_predictions, test_labels)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction A B C
##          A 7 0 0
##          B 0 6 1
##          C 0 0 6
## 
## Overall Statistics
##                                           
##                Accuracy : 0.95            
##                  95% CI : (0.7513, 0.9987)
##     No Information Rate : 0.35            
##     P-Value [Acc &gt; NIR] : 2.903e-08       
##                                           
##                   Kappa : 0.9251          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C
## Sensitivity              1.00   1.0000   0.8571
## Specificity              1.00   0.9286   1.0000
## Pos Pred Value           1.00   0.8571   1.0000
## Neg Pred Value           1.00   1.0000   0.9286
## Prevalence               0.35   0.3000   0.3500
## Detection Rate           0.35   0.3000   0.3000
## Detection Prevalence     0.35   0.3500   0.3000
## Balanced Accuracy        1.00   0.9643   0.9286</code></pre>
<p>At the top of the output, the confusion matrix shows how ‘mixed’ up the model got. Read it down the columns, so that for the 7 real group <code>A</code> the algorithm predicted 7 <code>A</code>, 0 <code>B</code> and 0 <code>C</code>; for the 6 real group <code>B</code> the algorithm predicted 0 <code>A</code>, 6 <code>B</code> and 0 <code>C</code> and for the 7 real group <code>C</code> the predictions were 0 <code>A</code>, 1 <code>B</code> and 6 <code>C</code>, so a <code>C</code> was misclassified as a <code>B</code>. This error rate and pattern is reflected in the overall accuracy, stated as 95 % and the more useful per group Sensitivity and Specificity, the lower Specificity for group <code>B</code> is due to the <code>C</code> miscalled as a <code>B</code> (so a false positive <code>B</code>). The same error causes the lower Sensitivity for group <code>C</code>.</p>
</div>
<div id="using-a-trained-model" class="section level3">
<h3><span class="header-section-number">5.5.2</span> Using a trained model</h3>
<p>Now that we have evaluated the model and know how accurate it is - and that it is accurate enough to be useful, we can run on our unknown data. This is virtually identical to before, replacing the <code>test_data</code> with the <code>unknown_data</code>. We must remember to reset the random number generator again, and we can go ahead and add the predictions straight to the data frame if we wish. We now have predicted groups for the unknown data and an estimate of the accuracy of our predictions.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="supervised-learning.html#cb60-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb60-2"><a href="supervised-learning.html#cb60-2"></a>unknown_predictions &lt;-<span class="st"> </span><span class="kw">knn</span>(train_data, <span class="dt">test=</span>unknown_data, <span class="dt">cl =</span> train_labels, <span class="dt">k=</span><span class="dv">9</span>)</span>
<span id="cb60-3"><a href="supervised-learning.html#cb60-3"></a>unknown_data<span class="op">$</span>predicted_group &lt;-<span class="st"> </span>unknown_predictions</span>
<span id="cb60-4"><a href="supervised-learning.html#cb60-4"></a>dplyr<span class="op">::</span><span class="kw">glimpse</span>(unknown_data)</span></code></pre></div>
<pre><code>## Rows: 75
## Columns: 5
## $ measure1        &lt;dbl&gt; 5.6, 5.0, 6.3, 6.1, 5.8, 5.5, 5.1, 5.1, 7.2, 5.0, 4.7,…
## $ measure2        &lt;dbl&gt; 2.9, 3.6, 2.8, 2.8, 2.7, 2.6, 3.8, 3.7, 3.0, 3.0, 3.2,…
## $ measure3        &lt;dbl&gt; 3.6, 1.4, 5.1, 4.7, 5.1, 4.4, 1.5, 1.5, 5.8, 1.6, 1.6,…
## $ measure4        &lt;dbl&gt; 1.3, 0.2, 1.5, 1.2, 1.9, 1.2, 0.3, 0.4, 1.6, 0.2, 0.2,…
## $ predicted_group &lt;fct&gt; B, A, C, B, C, B, A, A, C, A, A, C, A, C, B, B, C, A, …</code></pre>
</div>
</div>
<div id="random-forest" class="section level2">
<h2><span class="header-section-number">5.6</span> Random Forest</h2>
<p>Random Forest is another supervised learning algorithm that is based on ensembles of decision trees. A decision tree is a model that resembles a question flowchart that has a ‘question’ at each branch point and continues until enough have been ‘asked’ to differentiate the item in hand. Here is one potential decision tree for the animal classification we’ve been using.</p>
<p><img src="figs/dt.png" /><!-- --></p>
<p>In a Random Forest classifier trees are made using the training data and the ones that are best at classifying the data are retained. There are a whole set of possible good trees so the ensemble of trees is used, hence Random Forest. The many trees make up one model that are used with unseen data.</p>
<div id="building-a-random-forest-model" class="section level3">
<h3><span class="header-section-number">5.6.1</span> Building a Random Forest Model</h3>
<p>We use the <code>randomForest</code> package to do this, and we will use the training and test data as we did with <span class="math inline">\(k\)</span> nearest neighbours above, for random forest, the labels are specified in the data, so we don’t have a separate label vector and must now add them on to the training and test data. Let’s do that first</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="supervised-learning.html#cb62-1"></a>train_data<span class="op">$</span>group &lt;-<span class="st"> </span>train_labels</span>
<span id="cb62-2"><a href="supervised-learning.html#cb62-2"></a>dplyr<span class="op">::</span><span class="kw">glimpse</span>(train_data)</span></code></pre></div>
<pre><code>## Rows: 55
## Columns: 5
## $ measure1 &lt;dbl&gt; 6.7, 5.4, 6.4, 5.0, 5.3, 5.4, 5.4, 5.7, 6.0, 5.1, 6.0, 5.8, 4…
## $ measure2 &lt;dbl&gt; 3.1, 3.4, 3.2, 3.2, 3.7, 3.9, 3.0, 4.4, 2.9, 3.8, 2.2, 2.6, 3…
## $ measure3 &lt;dbl&gt; 5.6, 1.7, 5.3, 1.2, 1.5, 1.7, 4.5, 1.5, 4.5, 1.9, 4.0, 4.0, 1…
## $ measure4 &lt;dbl&gt; 2.4, 0.2, 2.3, 0.2, 0.2, 0.4, 1.5, 0.4, 1.5, 0.4, 1.0, 1.2, 0…
## $ group    &lt;fct&gt; C, A, C, A, A, A, B, A, B, A, B, B, A, A, B, C, C, A, A, B, A…</code></pre>
<p>We can now build the model with the <code>randomForest()</code> function. The setup uses R’s formula based syntax, so is very similar to that we used for linear models. The <code>group</code> is to be predicted based on <code>.</code> which means all other columns in the data <code>train_data</code>. The <code>model</code> variable holds the trained model</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="supervised-learning.html#cb64-1"></a><span class="kw">library</span>(randomForest)</span>
<span id="cb64-2"><a href="supervised-learning.html#cb64-2"></a>model &lt;-<span class="st"> </span><span class="kw">randomForest</span>(group <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train_data, <span class="dt">mtry=</span><span class="dv">2</span>)</span></code></pre></div>
</div>
<div id="testing-a-random-forest-model" class="section level3">
<h3><span class="header-section-number">5.6.2</span> Testing a Random Forest model</h3>
<p>With the model built we can use the generic <code>predict()</code> function to get the model to predict groups for the unlabelled <code>test_data</code> then compare it to the real groups with <code>confusionMatrix()</code>. Setting the value of <code>type</code> to <code>class</code> tells the <code>predict()</code> we want group classifications</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="supervised-learning.html#cb65-1"></a>test_set_predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(model, test_data, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</span>
<span id="cb65-2"><a href="supervised-learning.html#cb65-2"></a><span class="kw">confusionMatrix</span>(test_set_predictions, test_labels)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction A B C
##          A 7 0 0
##          B 0 6 1
##          C 0 0 6
## 
## Overall Statistics
##                                           
##                Accuracy : 0.95            
##                  95% CI : (0.7513, 0.9987)
##     No Information Rate : 0.35            
##     P-Value [Acc &gt; NIR] : 2.903e-08       
##                                           
##                   Kappa : 0.9251          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C
## Sensitivity              1.00   1.0000   0.8571
## Specificity              1.00   0.9286   1.0000
## Pos Pred Value           1.00   0.8571   1.0000
## Neg Pred Value           1.00   1.0000   0.9286
## Prevalence               0.35   0.3000   0.3500
## Detection Rate           0.35   0.3000   0.3000
## Detection Prevalence     0.35   0.3500   0.3000
## Balanced Accuracy        1.00   0.9643   0.9286</code></pre>
<p>The model is again, convincing and highly accurate so we can repeat use <code>predict()</code> with <code>model</code> to get predictions for <code>unknown_data</code>, and again add it to the data</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="supervised-learning.html#cb67-1"></a>unknown_predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(model, unknown_data, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</span>
<span id="cb67-2"><a href="supervised-learning.html#cb67-2"></a>unknown_data<span class="op">$</span>predicted_group &lt;-<span class="st"> </span>unknown_predictions</span>
<span id="cb67-3"><a href="supervised-learning.html#cb67-3"></a>dplyr<span class="op">::</span><span class="kw">glimpse</span>(unknown_data)</span></code></pre></div>
<pre><code>## Rows: 75
## Columns: 5
## $ measure1        &lt;dbl&gt; 5.6, 5.0, 6.3, 6.1, 5.8, 5.5, 5.1, 5.1, 7.2, 5.0, 4.7,…
## $ measure2        &lt;dbl&gt; 2.9, 3.6, 2.8, 2.8, 2.7, 2.6, 3.8, 3.7, 3.0, 3.0, 3.2,…
## $ measure3        &lt;dbl&gt; 3.6, 1.4, 5.1, 4.7, 5.1, 4.4, 1.5, 1.5, 5.8, 1.6, 1.6,…
## $ measure4        &lt;dbl&gt; 1.3, 0.2, 1.5, 1.2, 1.9, 1.2, 0.3, 0.4, 1.6, 0.2, 0.2,…
## $ predicted_group &lt;fct&gt; B, A, C, B, C, B, A, A, C, A, A, C, A, C, B, B, C, A, …</code></pre>
</div>
<div id="random-forest-with-categorical-predictors" class="section level3">
<h3><span class="header-section-number">5.6.3</span> Random Forest with categorical predictors</h3>
<p>In our <span class="math inline">\(k\)</span>NN example and the previous Random Forest predictor, the input data features were solely numeric. Random Forest can handle a mixture of numeric and character or categorical based features allowing us to make classifications on more than numbers. The process is similar, so let’s get some appropriate data and do that</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="supervised-learning.html#cb69-1"></a>dplyr<span class="op">::</span><span class="kw">glimpse</span>(train_data_mixed)</span></code></pre></div>
<pre><code>## Rows: 55
## Columns: 6
## $ measure1 &lt;dbl&gt; 6.7, 5.4, 6.4, 5.0, 5.3, 5.4, 5.4, 5.7, 6.0, 5.1, 6.0, 5.8, 4…
## $ measure2 &lt;dbl&gt; 3.1, 3.4, 3.2, 3.2, 3.7, 3.9, 3.0, 4.4, 2.9, 3.8, 2.2, 2.6, 3…
## $ measure3 &lt;dbl&gt; 5.6, 1.7, 5.3, 1.2, 1.5, 1.7, 4.5, 1.5, 4.5, 1.9, 4.0, 4.0, 1…
## $ measure4 &lt;dbl&gt; 2.4, 0.2, 2.3, 0.2, 0.2, 0.4, 1.5, 0.4, 1.5, 0.4, 1.0, 1.2, 0…
## $ group    &lt;fct&gt; C, A, C, A, A, A, B, A, B, A, B, B, A, A, B, C, C, A, A, B, A…
## $ colour   &lt;fct&gt; White, Green, White, Green, Green, Green, Blue, Green, Blue, …</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="supervised-learning.html#cb71-1"></a>dplyr<span class="op">::</span><span class="kw">glimpse</span>(test_data_mixed)</span></code></pre></div>
<pre><code>## Rows: 20
## Columns: 6
## $ measure1 &lt;dbl&gt; 5.6, 6.7, 6.3, 6.3, 5.0, 7.2, 6.2, 6.7, 4.6, 5.1, 6.0, 6.7, 7…
## $ measure2 &lt;dbl&gt; 3.0, 2.5, 3.3, 2.7, 2.3, 3.6, 2.2, 3.1, 3.4, 3.8, 2.2, 3.3, 3…
## $ measure3 &lt;dbl&gt; 4.1, 5.8, 6.0, 4.9, 3.3, 6.1, 4.5, 4.7, 1.4, 1.6, 5.0, 5.7, 6…
## $ measure4 &lt;dbl&gt; 1.3, 1.8, 2.5, 1.8, 1.0, 2.5, 1.5, 1.5, 0.3, 0.2, 1.5, 2.1, 2…
## $ group    &lt;fct&gt; B, C, C, C, B, C, B, B, A, A, C, C, C, A, A, B, A, A, B, A
## $ colour   &lt;fct&gt; Blue, White, White, White, Blue, White, Blue, Blue, Green, Gr…</code></pre>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="supervised-learning.html#cb73-1"></a>dplyr<span class="op">::</span><span class="kw">glimpse</span>(unknown_data_mixed)</span></code></pre></div>
<pre><code>## Rows: 75
## Columns: 6
## $ measure1 &lt;dbl&gt; 5.6, 5.0, 6.3, 6.1, 5.8, 5.5, 5.1, 5.1, 7.2, 5.0, 4.7, 7.7, 5…
## $ measure2 &lt;dbl&gt; 2.9, 3.6, 2.8, 2.8, 2.7, 2.6, 3.8, 3.7, 3.0, 3.0, 3.2, 2.8, 3…
## $ measure3 &lt;dbl&gt; 3.6, 1.4, 5.1, 4.7, 5.1, 4.4, 1.5, 1.5, 5.8, 1.6, 1.6, 6.7, 1…
## $ measure4 &lt;dbl&gt; 1.3, 0.2, 1.5, 1.2, 1.9, 1.2, 0.3, 0.4, 1.6, 0.2, 0.2, 2.0, 0…
## $ colour   &lt;fct&gt; Blue, Green, White, Blue, White, Blue, Green, Green, White, G…
## $ group    &lt;fct&gt; B, A, C, B, C, B, A, A, C, A, A, C, A, C, B, B, C, A, C, B, A…</code></pre>
<p>We can see that there is a new categorical feature called <code>colour</code> in our train and test data, but not in our unknown data, so let’s try to predict the colour this time.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="supervised-learning.html#cb75-1"></a>model2 &lt;-<span class="st"> </span><span class="kw">randomForest</span>(colour <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train_data_mixed, <span class="dt">mtry=</span><span class="dv">2</span>)</span>
<span id="cb75-2"><a href="supervised-learning.html#cb75-2"></a></span>
<span id="cb75-3"><a href="supervised-learning.html#cb75-3"></a>test_set_mixed_predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(model2, test_data_mixed, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</span>
<span id="cb75-4"><a href="supervised-learning.html#cb75-4"></a><span class="kw">confusionMatrix</span>(test_set_mixed_predictions, test_labels_mixed)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction Blue Green White
##      Blue     6     0     0
##      Green    0     7     0
##      White    0     0     7
## 
## Overall Statistics
##                                      
##                Accuracy : 1          
##                  95% CI : (0.8316, 1)
##     No Information Rate : 0.35       
##     P-Value [Acc &gt; NIR] : 7.61e-10   
##                                      
##                   Kappa : 1          
##                                      
##  Mcnemar&#39;s Test P-Value : NA         
## 
## Statistics by Class:
## 
##                      Class: Blue Class: Green Class: White
## Sensitivity                  1.0         1.00         1.00
## Specificity                  1.0         1.00         1.00
## Pos Pred Value               1.0         1.00         1.00
## Neg Pred Value               1.0         1.00         1.00
## Prevalence                   0.3         0.35         0.35
## Detection Rate               0.3         0.35         0.35
## Detection Prevalence         0.3         0.35         0.35
## Balanced Accuracy            1.0         1.00         1.00</code></pre>
<p>So we have created a model that is capable of perfectly predicting the value of the categoric value colour from a mixture of numeric and categoric features. Why is the model so accurate? It’s a bit of a fix! This sample data has a direct mapping between the <code>group</code> and the <code>colour</code>: <code>A</code> is always <code>Green</code>, <code>B</code> is always <code>Blue</code> and <code>C</code> is always <code>White</code> so it is easy to predict colour if you have <code>group</code>. The data aren’t typical in this sense but it does highlight the procedure.</p>
</div>
<div id="random-forest-regression" class="section level3">
<h3><span class="header-section-number">5.6.4</span> Random Forest Regression</h3>
<p>It is also possible to perform prediction of numeric values and not just classes with Random Forest. We simply set up the model with a numeric value as the predicted value in the formula as follows</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="supervised-learning.html#cb77-1"></a>model3 &lt;-<span class="st"> </span><span class="kw">randomForest</span>(measure1 <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train_data_mixed, <span class="dt">mtry=</span><span class="dv">2</span>)</span></code></pre></div>
<p>Now when we use <code>predict()</code> and omit the <code>type</code> argument, we get a set of numbers, not classes back</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="supervised-learning.html#cb78-1"></a>test_set_mixed_predictions_numeric &lt;-<span class="st"> </span><span class="kw">predict</span>(model3, test_data_mixed)</span>
<span id="cb78-2"><a href="supervised-learning.html#cb78-2"></a>test_set_mixed_predictions_numeric</span></code></pre></div>
<pre><code>##       89      109      101      124       94      110       69       87 
## 5.966036 6.237482 6.900857 6.007691 5.517170 6.887034 5.695914 6.222375 
##        7       47      120      125      118        1       15       96 
## 5.039731 5.181784 5.838623 6.722102 6.891138 5.088806 5.109832 5.982576 
##       24       38       88       48 
## 5.263310 5.012233 5.735793 4.833798</code></pre>
<div id="evaluating-numeric-predictions" class="section level4">
<h4><span class="header-section-number">5.6.4.1</span> Evaluating numeric predictions</h4>
<p>Previously we’ve evaluated predictions from our models for classes, counting True Positives etc, but we can’t do that here because we have no classes. Instead we can calculate how far away from the real value the predictions are on average. That’s a simple sum to do in R</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="supervised-learning.html#cb80-1"></a><span class="kw">mean</span>( (test_data_mixed<span class="op">$</span>measure1 <span class="op">-</span><span class="st"> </span>test_set_mixed_predictions_numeric) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.1745713</code></pre>
<p>The quantity is called the Mean Squared Error or MSE. The lower the better, though the actual size is dependent on context. The context here is the descriptive statistics of the known values for the test data, which we can get with <code>summary()</code></p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="supervised-learning.html#cb82-1"></a><span class="kw">summary</span>(test_data_mixed<span class="op">$</span>measure1)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    4.60    5.10    5.90    5.88    6.40    7.70</code></pre>
<p>These values range between 4.6 and 7.7, with 50% of them lying between 5.1 and 6.4. With that in mind it seems like an MSE of 0.17 is a pretty good result and we can conclude to predict accurately the values of <code>measure1</code> from our Random Forest Regression model.</p>

<div class="roundup">
<ul>
<li>Supervised Learning uses labelled data to make predictions on unseen data</li>
<li>Random Forest can predict classes and numeric values (perform regression)</li>
<li>It is imperative to evaluate the predictive model on a set of known cases</li>
</ul>
</div>


</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="unsupervised-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="deep-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-supervised.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["machine_learning.pdf", "machine_learning.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
